running ./MaxFlops_double microbenchmark
DPU FLOP per SM = 3.999892 (flop/clk/SM)
Total Clk number = 8388835 
/////////////////////////////////
running ./MaxFlops_float microbenchmark
FLOP per SM = 126.857941 (flop/clk/SM)
Total Clk number = 66126 
/////////////////////////////////
running ./MaxFlops_half microbenchmark
half FLOP per SM = 251.623016 (flop/clk/SM)
Total Clk number = 16669 
/////////////////////////////////
running ./MaxFlops_int32 microbenchmark
int32 FLOP per SM = 126.796577 (flop/clk/SM)
Total Clk number = 66158 
/////////////////////////////////
running ./atomic_add_bw microbenchmark
Atomic int32 bandwidth = 22.395056 (byte/clk)
Total Clk number = 47945485 
/////////////////////////////////
running ./atomic_add_bw_conflict microbenchmark
Atomic int32 bandwidth = 127.304337 (byte/clk)
Total Clk number = 527153 
/////////////////////////////////
running ./atomic_add_lat microbenchmark
Atomic int32 latency = 160.584961 (clk)
Total Clk number = 164439 
/////////////////////////////////
running ./config_dpu microbenchmark
DPU FLOP per SM = 3.999913 (flop/clk/SM)
Total Clk number = 8388790 
double-precision DPU latency = 53.031067 (clk)
Total Clk number = 868861 

//Accel_Sim config: 
-gpgpu_num_dp_units 4
-ptx_opcode_latency_dp 53,53,53,53,330
-ptx_opcode_initiation_dp 64,64,64,64,130
-trace_opcode_latency_initiation_dp 53,64
/////////////////////////////////
running ./config_fpu microbenchmark
FLOP per SM = 126.844513 (flop/clk/SM)
Total Clk number = 66133 
float-precision FPU latency = 4.229614 (clk)
Total Clk number = 69298 

//Accel_Sim config: 
-gpgpu_num_sp_units 4
-ptx_opcode_latency_fp 4,4,4,4,39
-ptx_opcode_initiation_fp 2,2,2,2,4
-trace_opcode_latency_initiation_sp 4,2
/////////////////////////////////
running ./config_int microbenchmark
int32 FLOP per SM = 126.905914 (flop/clk/SM)
Total Clk number = 66101 
int32 latency = 4.729492 (clk)
Total Clk number = 19372 

//Accel_Sim config: 
-gpgpu_num_int_units 4
-ptx_opcode_latency_int 4,4,4,4,21
-ptx_opcode_initiation_int 2,2,2,2,2
-trace_opcode_latency_initiation_int 4,2
/////////////////////////////////
running ./config_sfu microbenchmark
SFU fast sqrt bw = 15.9857(flops/clk/SM) 
Total Clk number = 262378
SFU fast sqrt latency = 21.5081(clk) 
Total Clk number = 88097

//Accel_Sim config: 
-gpgpu_num_sfu_units 4
-ptx_opcode_latency_sfu 21
-ptx_opcode_initiation_sfu 8
-trace_opcode_latency_initiation_sfu 21,8
/////////////////////////////////
running ./config_tensor microbenchmark
wmma PTX issue bandwidth = 3.69014(thread/clk/SM) 
hmma SASS issue bandwidth = 59.0422(thread/clk/SM)
FMA tensor bandwidth = 472.337(FMA/clk/SM)
Total Clk number = 568313
wmma latency = 44.5723(clk)
hmma latency = 2.78577(clk)
Total Clk number = 182568

//Accel_Sim config: 
-gpgpu_tensor_core_avail 1
-gpgpu_num_tensor_core_units 4
-ptx_opcode_latency_tesnor 44
-ptx_opcode_initiation_tensor 32
-trace_opcode_latency_initiation_tensor 2,2
-specialized_unit_3 1,4,2,4,4,TENSOR
-trace_opcode_latency_initiation_spec_op_3 2,2
/////////////////////////////////
running ./config_udp microbenchmark
-specialized_unit_4 1,4,4,4,4,UDP
-trace_opcode_latency_initiation_spec_op_4 4,1
/////////////////////////////////
running ./core_config microbenchmark
CUDA version number = 7.2

//Accel_Sim config: 
-gpgpu_ptx_force_max_capability 72
-gpgpu_shader_registers 65536
-gpgpu_registers_per_block 65536
-gpgpu_occupancy_sm_number 72
-gpgpu_coalesce_arch 72
-gpgpu_pipeline_widths 4,4,4,4,4,4,4,4,4,4,8,4,4
-gpgpu_sub_core_model 1
-gpgpu_enable_specialized_operand_collector 0
-gpgpu_operand_collector_num_units_gen 8
-gpgpu_operand_collector_num_in_ports_gen 8
-gpgpu_operand_collector_num_out_ports_gen 8
-gpgpu_num_sched_per_core 4
-gpgpu_max_insn_issue_per_warp 1
-gpgpu_dual_issue_diff_exec_units 1
-gpgpu_inst_fetch_throughput 4
-gpgpu_shader_core_pipeline 2048:32
-gpgpu_shader_cta 32
/////////////////////////////////
running ./deviceQuery microbenchmark
  Device : "Xavier"

  CUDA version number                         : 7.2
  GPU Max Clock rate                             : 1377 MHz 
  Multiprocessors Count                       : 8
  Maximum number of threads per multiprocessor: 2048
  CUDA Cores per multiprocessor               : 64 
  Registers per multiprocessor                : 65536
  Shared memory per multiprocessor            : 98304 bytes
  Warp size                                   : 32
  Maximum number of threads per block         : 1024
  Shared memory per block                     : 49152 bytes
  Registers per block                         : 65536
  globalL1CacheSupported                      : 1
  localL1CacheSupported                       : 1
  L2 Cache Size                             : 0 MB
  Global memory size                        : 30 GB
  Memory Clock rate                           : 2133 Mhz
  Memory Bus Width                            : 256 bit
 ////////////////////////// 
/////////////////////////////////
running ./kernel_lat microbenchmark
Kernel Launch Latency = 23089.5 cycles
The reported latency above can be slightly higher than real. For accurate evaultion using nvprof event, exmaple: make events ./kernel_lat

//Accel_Sim config: 
-gpgpu_kernel_launch_latency  23089
/////////////////////////////////
running ./l1_access_grain microbenchmark

This benchmark measures coalescing granularity for differnet strides.
check the nvprof or nvsight for received l1 reads and writes.
to run the program with nsight: make nvsight ./l1_access_grain
stats to look at: l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum

/////////////////////////////////
running ./l1_adaptive microbenchmark
The ubench is not imepleneted yet.
/////////////////////////////////
running ./l1_associativity microbenchmark
Launching L1 cache line size ubench
Saving L1 cache line size data at L1line.csv
Launching L1 cache assoc ubench
Saving L1 cache assoc data at L1asso.csv
/////////////////////////////////
running ./l1_banks microbenchmark
The ubench is not imepleneted yet.
/////////////////////////////////
running ./l1_bw_128 microbenchmark
L1 bandwidth = 117.356(byte/clk/SM), 150.501(GB/s/SM)
Total Clk number = 35740
/////////////////////////////////
running ./l1_bw_32f microbenchmark
L1 bandwidth = 78.9026(byte/clk/SM), 101.187(GB/s/SM)
Total Clk number = 53158
/////////////////////////////////
running ./l1_bw_32f_unroll microbenchmark
L1 bandwidth = 55.777546 (byte/clk/SM)
Total Clk number = 75197 
/////////////////////////////////
running ./l1_bw_64f microbenchmark
L1 bandwidth = 13.4595(byte/clk/SM), 17.2609(GB/s/SM)
Total Clk number = 311624
/////////////////////////////////
running ./l1_bw_64v microbenchmark
L1 bandwidth = 115.007(byte/clk/SM), 147.489(GB/s/SM)
Total Clk number = 18235
/////////////////////////////////
running ./l1_config microbenchmark

//Accel_Sim config: 
-gpgpu_adaptive_cache_config 1
-gpgpu_shmem_option 0,8,16,32,64,96
-gpgpu_unified_l1d_size 128
-gpgpu_l1_banks 4
-gpgpu_cache:dl1 S:4:128:64,L:T:m:L:L,A:512:64,16:0,32
-gpgpu_gmem_skip_L1D 0
-gpgpu_l1_cache_write_ratio 25
/////////////////////////////////
running ./l1_lat microbenchmark
L1 Latency  =      31.3903 cycles
Total Clk number = 1028598 

//Accel_Sim config: 
-gpgpu_l1_latency 31
/////////////////////////////////
running ./l1_mshr microbenchmark
Launching L1 MSHR ubench
Saving L1 MSHR data at MSHR100_array1073741824_shmem12288_itr6.csv
/////////////////////////////////
running ./l1_sector microbenchmark
Launching L1 sector ubench
Saving L1 sector data at data.csv
/////////////////////////////////
running ./l1_shared_bw microbenchmark
Shared Memory Bandwidth = 101.021316 (byte/clk/SM)
Total Clk number = 332152 
/////////////////////////////////
running ./l1_write_policy microbenchmark

This microbenchmark detects L1 write policy.
check the nvprof or nvsight for received l1 reads and writes to detect the policy.
see the code comments for further details
to run the program with nvsight: make nvsight ./l1_write_policy
stats to look at: l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_hit.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_hit.sum 

/////////////////////////////////
running ./l2_access_grain microbenchmark

This benchmark measures l2 access granularity for differnet strides.
check the nvprof or nvsight for received l2 reads and write.
to run the program with nsight: make nvsight ./l2_access_grain
stats to look at: lts__t_sectors_srcunit_tex_op_read.sum and lts__t_sectors_srcunit_tex_op_write.sum 

/////////////////////////////////
running ./l2_bw_128 microbenchmark
L2 bandwidth = 128.557(byte/clk), 164.866(GB/s)
Max Theortical L2 bandwidth = 512(byte/clk), 656.605(GB/s)
L2 BW achievable = 25.1089%
Total Clk number = 522015
/////////////////////////////////
running ./l2_bw_32f microbenchmark
L2 bandwidth = 127.843(byte/clk), 163.949(GB/s)
Max Theortical L2 bandwidth = 512(byte/clk), 656.605(GB/s)
L2 BW achievable = 24.9693%
Total Clk number = 1049866
/////////////////////////////////
running ./l2_bw_64f microbenchmark
/////////////////////////////////
running ./l2_config microbenchmark
L2 Cache Size = 0 MB
L2 Banks number = 16

//Accel_Sim config: 
-gpgpu_n_sub_partition_per_mchannel 2
-icnt_flit_size 40
-gpgpu_memory_partition_indexing 2
-gpgpu_cache:dl2 S:16:128:16,L:B:m:L:P,A:192:4,32:0,32
/////////////////////////////////
running ./l2_copy_engine microbenchmark
L2 Latency no-warmp up =     138.0602 cycles 
Total Clk number = 4523956 
L2 Hit Latency =     138.1238 cycles 
Total Clk number = 4526042 
Is memcpy cached in L2? Yes, error=0.0

//Accel_Sim config: 
-gpgpu_perf_sim_memcpy 1
/////////////////////////////////
running ./l2_lat microbenchmark
L2 Hit Latency =     137.7488 cycles 
Total Clk number = 4513754 
L1 Latency  =      31.4368 cycles
Total Clk number = 1030122 

//Accel_Sim config: 
-gpgpu_l2_rop_latency 105
/////////////////////////////////
running ./l2_write_policy microbenchmark

This microbenchmark detects L2 write policy.
check the nvprof or nvsight for received L2 reads and writes to detect the policy.
see the code comments for further details
to run the program with nvsight: make nvsight ./2
stats to look at: llts__t_sectors_srcunit_tex_op_read.sum & lts__t_sectors_srcunit_tex_op_write.sum & lts__t_sectors_srcunit_tex_op_read_lookup_hit.sum & lts__t_sectors_srcunit_tex_op_write_lookup_hit.sum 

/////////////////////////////////
running ./lat_double microbenchmark
double-precision DPU latency = 53.038330 (clk)
Total Clk number = 868980 
/////////////////////////////////
running ./lat_float microbenchmark
float-precision FPU latency = 4.228638 (clk)
Total Clk number = 69282 
/////////////////////////////////
running ./lat_half microbenchmark
fpu16 latency = 13.997070 (clk)
Total Clk number = 57332 
/////////////////////////////////
running ./lat_int32 microbenchmark
int32 latency = 4.745361 (clk)
Total Clk number = 19437 
/////////////////////////////////
running ./list_devices microbenchmark

Device 0: "Xavier sm_7.2"
/////////////////////////////////
running ./mem_atom_size microbenchmark

This benchmark measures mem atom size granularity
check the nvprof or nvsight for received mem reads and writes
to run the program with nsight: make nvsight ./l2_access_grain
stats to look at: dram__sectors_read.sum & dram__sectors_write.sum & dram__bytes_read.sum & dram__sectors_read.sum

we launched 262144 read memory reqs (1 req per thread) with a stride of 32 (128 bytes)
if the number of memory reads is the same as read reqs, then mem atom size is 32B
if the number of memory reads is 2X issued read reqs, then mem atom size is 64B, etc.

/////////////////////////////////
running ./mem_bw microbenchmark
Mem BW= 179.279510 (Byte/Clk)
Mem BW= 71.028904 (GB/sec)
Max Theortical Mem BW= 136.511993 (GB/sec)
Mem Efficiency = 131.328766 %
Total Clk number = 35093 
/////////////////////////////////
running ./mem_config microbenchmark
Global memory size = 30 GB
Memory Clock rate = 2133 Mhz
Memory Bus Width = 256 bit
Memory type = LPDDR4
Memory channels = 8

//Accel_Sim config: 
-gpgpu_n_mem 8
-gpgpu_n_mem_per_ctrlr 2
-gpgpu_dram_buswidth 2
-gpgpu_dram_burst_length 16
-dram_data_command_freq_ratio 2
-dram_dual_bus_interface 0
-gpgpu_dram_timing_opt nbk=8:CCD=8:RRD=22:RCD=39:RAS=91:RP=40:RC=127:CL=16:WL=19:CDLR=22:WR=38:nbkgrp=1:CCDL=0:RTPL=0
/////////////////////////////////
running ./mem_lat microbenchmark
Mem latency =     575.2858 cycles 
Total Clk number = 4712741 
L2 Hit Latency =     138.1238 cycles 
Total Clk number = 4526042 

//Accel_Sim config: 
-dram_latency 437
/////////////////////////////////
running ./regfile_bw microbenchmark
wmma PTX issue bandwidth = 3.68444(thread/clk/SM) 
hmma SASS issue bandwidth = 58.9511(thread/clk/SM)
FMA tensor bandwidth = 471.609(FMA/clk/SM)
Total Clk number = 569191

regfile_bw = 2048 (byte/SM)

//Accel_Sim config: 
-gpgpu_num_reg_banks 16
-gpgpu_reg_file_port_throughput 2
/////////////////////////////////
running ./sfu_bw_fsqrt microbenchmark
SFU fast sqrt bw = 15.9845(flops/clk/SM) 
Total Clk number = 262399
/////////////////////////////////
running ./sfu_lat_fsqrt microbenchmark
SFU fast sqrt latency = 21.6343(clk) 
Total Clk number = 88614
/////////////////////////////////
running ./shared_bw microbenchmark
Shared Memory Bandwidth = 126.222(byte/clk/SM), 161.871(GB/s/SM)
Total Clk number = 132918
/////////////////////////////////
running ./shared_bw_64 microbenchmark
Shared Memory Bandwidth = 127.783(byte/clk/SM), 163.872(GB/s/SM)
Total Clk number = 262590
/////////////////////////////////
running ./shared_lat microbenchmark
Shared Memory Latency  = 27.013184 cycles
Total Clk number = 55323 

//Accel_Sim config: 
-gpgpu_smem_latency 27
/////////////////////////////////
running ./shd_config microbenchmark
Shared memory per multiprocessor = 98304 bytes
Shared memory per block = 49152 bytes

//Accel_Sim config: 
-gpgpu_shmem_size 98304
-gpgpu_shmem_sizeDefault 98304
-gpgpu_shmem_per_block 49152
/////////////////////////////////
running ./system_config microbenchmark
Device Name = Xavier
GPU Max Clock rate = 1377 MHz 
GPU Base Clock rate = 1377 MHz 
SM Count = 8
CUDA version number = 7.2

//Accel_Sim config: 
-gpgpu_compute_capability_major 7
-gpgpu_compute_capability_minor 2
-gpgpu_n_clusters 8
-gpgpu_n_cores_per_cluster 1
-gpgpu_clock_domains 1377:1377:1377:2133
/////////////////////////////////
running ./tensor_bw_half microbenchmark
FP16 operand, FP32 accumalte:
wmma PTX issue bandwidth = 3.66358(thread/clk/SM) 
hmma SASS issue bandwidth = 58.6172(thread/clk/SM)
FMA tensor bandwidth = 468.938(FMA/clk/SM)
Total Clk number = 572433

FP16 operand, FP16 accumalte:
wmma PTX issue bandwidth = 3.98566(thread/clk/SM) 
hmma SASS issue bandwidth = 63.7706(thread/clk/SM)
FMA tensor bandwidth = 510.165(FMA/clk/SM)
Total Clk number = 526174
/////////////////////////////////
running ./tensor_lat_half microbenchmark
FP16 operand, FP32 accumalte:
wmma latency = 45.0061(clk)
hmma latency = 2.81288(clk)
Total Clk number = 184345

FP16 operand, FP16 accumalte:
wmma latency = 33.3604(clk)
hmma latency = 2.08502(clk)
Total Clk number = 136644
/////////////////////////////////
